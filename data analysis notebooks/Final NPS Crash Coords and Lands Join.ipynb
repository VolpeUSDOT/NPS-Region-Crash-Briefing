{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "006183d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If running in Anaconda/Jupyter Notebook, create a new anaconda environment and install geopandas, otherwise it won't run\n",
    "\n",
    "#Using NPS Lands Layer Package\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "    \n",
    "ALLCRASHES = pd.read_csv(r\"C:\\Users\\Christopher.Dettmer\\Documents\\TSP\\Using New Data\\IMARS_crash_expanded_8-25-22.csv\")\n",
    "ALLBOUNDARIES=gpd.read_file(r\"C:\\Users\\Christopher.Dettmer\\Documents\\TSP\\Spatial Join Files\\nps_boundary.shp\")\n",
    "\n",
    "def NCR_Cleaner(ALLCRASHES):\n",
    "    \n",
    "    #This is the list of parks that (1) are not assigned the NACA label in the NPS Lands Boundary and\n",
    "    #(2) also have crashes with a park and coordinate filled; may not be entirely complete but should cover\n",
    "    #any parks that would be in the top 15 for the NCR\n",
    "    \n",
    "    nonNACAlist=[\"GWMP\",\"ROCR\",\"LINC\",\"MANA\",\"JEFM\",\"CATO\",\"PRWI\",\"CLBA\",\"THIS\",\"COGA\",\"FRDE\",\"GREE\",\n",
    "                 \"ANTI\",\"PAAV\",\"PISC\",\"MONO\",\"WHHO\",\"FOWA\",\"HAFE\",\"ARHO\",\"WOTR\",\"MLKM\",\"DDEM\",\"CHOH\",\n",
    "                 \"BEPA\",\"LYBA\",\"MALL\",\"MABE\",\"WWII\",\"FOTH\",\"WAMO\",\"KOWA\",\"FRDO\",\"CAWO\",\"VIVE\",\"WWIM\"]\n",
    "    \n",
    "    for crash in range(len(ALLCRASHES)):\n",
    "        park=ALLCRASHES.iloc[crash][1]\n",
    "        region=ALLCRASHES.iloc[crash][4]\n",
    "        \n",
    "        if park not in nonNACAlist and region==\"NC\":\n",
    "            ALLCRASHES.iat[crash,1]=\"NACA\"\n",
    "            \n",
    "    return ALLCRASHES\n",
    "\n",
    "\n",
    "def crashChooser(ALLCRASHES_CLEAN,ALLBOUNDARIES,parkCode):\n",
    "        \n",
    "    #Take park crashes, turn into a dataframe with coords, change from geometric to projected coords for sjoin\n",
    "    park_crashes_df=ALLCRASHES_CLEAN.loc[ALLCRASHES_CLEAN['Park']==parkCode]\n",
    "    park_crashes=gpd.GeoDataFrame(park_crashes_df, geometry=gpd.points_from_xy(park_crashes_df.Longitude,park_crashes_df.Latitude))\n",
    "    proj_park_crashes=park_crashes.set_crs(epsg=3857)\n",
    "    \n",
    "    return proj_park_crashes\n",
    "\n",
    "def boundaryChooser(ALLCRASHES_CLEAN,ALLBOUNDARIES,parkCode):\n",
    "    \n",
    "    #Take park boundary(ies), change from geometric to projected coords for sjoin\n",
    "    park_polygon=ALLBOUNDARIES.loc[ALLBOUNDARIES['UNIT_CODE']==parkCode]\n",
    "    proj_park_polygon=park_polygon.set_crs(epsg=3857,allow_override=True)          \n",
    "        \n",
    "    return proj_park_polygon\n",
    "    \n",
    "\n",
    "def sjoin_0(proj_park_crashes_clean, proj_park_polygon):\n",
    "    \n",
    "    return gpd.sjoin(proj_park_polygon,proj_park_crashes_clean,how='left')\n",
    "\n",
    "\n",
    "def sjoin_1(proj_park_crashes_clean, proj_park_polygon):\n",
    "    \n",
    "    #Take park boundary(ies) with projected coords, add buffer, then reformat to geodataseries\n",
    "    park_polygon_1_buffer_geoseries=gpd.GeoSeries.buffer(proj_park_polygon,0.0145055773)\n",
    "    park_polygon_1_buffer=gpd.GeoDataFrame(geometry=gpd.GeoSeries(park_polygon_1_buffer_geoseries))\n",
    "\n",
    "    return gpd.sjoin(park_polygon_1_buffer,proj_park_crashes_clean,how='left')\n",
    "\n",
    "\n",
    "def sjoin_10(proj_park_crashes_clean, proj_park_polygon):\n",
    "    \n",
    "    #Take park boundary(ies) with projected coords, add buffer, then reformat to geodataseries\n",
    "    park_polygon_10_buffer_geoseries=gpd.GeoSeries.buffer(proj_park_polygon,0.1450557739)\n",
    "    park_polygon_10_buffer=gpd.GeoDataFrame(geometry=gpd.GeoSeries(park_polygon_10_buffer_geoseries))\n",
    "\n",
    "    return gpd.sjoin(park_polygon_10_buffer,proj_park_crashes_clean,how='left')\n",
    "\n",
    "\n",
    "def sjoin_100(proj_park_crashes_clean, proj_park_polygon):\n",
    "    \n",
    "    #Take park boundary(ies) with projected coords, add buffer, then reformat to geodataseries\n",
    "    park_polygon_100_buffer_geoseries=gpd.GeoSeries.buffer(proj_park_polygon,1.45055774)\n",
    "    park_polygon_100_buffer=gpd.GeoDataFrame(geometry=gpd.GeoSeries(park_polygon_100_buffer_geoseries))\n",
    "\n",
    "    return gpd.sjoin(park_polygon_100_buffer,proj_park_crashes_clean,how='left')\n",
    "\n",
    "\n",
    "def noCoordsCleaner(proj_park_crashes):\n",
    "\n",
    "    noCoords=0\n",
    "    \n",
    "    for crash in range(len(proj_park_crashes)):\n",
    "        \n",
    "        if pd.isnull(proj_park_crashes.iloc[crash][2])==True or pd.isnull(proj_park_crashes.iloc[crash][3])==True:    \n",
    "            noCoords+=1\n",
    "            #proj_park_crashes.drop(crash)\n",
    "            \n",
    "    return proj_park_crashes, noCoords\n",
    "    \n",
    "\n",
    "def calculations(proj_park_crashes, proj_park_polygon, outputDataFrame, output_df_park, output_df_region):\n",
    "    \n",
    "    proj_park_crashes_clean, noCoords=noCoordsCleaner(proj_park_crashes)\n",
    "    \n",
    "    within0=len(sjoin_0(proj_park_crashes_clean, proj_park_polygon))\n",
    "    within1=len(sjoin_1(proj_park_crashes_clean, proj_park_polygon))\n",
    "    within10=len(sjoin_10(proj_park_crashes_clean, proj_park_polygon))\n",
    "    within100=len(sjoin_100(proj_park_crashes_clean, proj_park_polygon))\n",
    "    \n",
    "    #Unable to drop crashes without coordinates in noCoordsCleaner, receiving errors\n",
    "    #Workaround by subtracting crashes without coordinates from crashes over 100 miles outside of park boundary\n",
    "    \n",
    "    totalCrashes=len(proj_park_crashes_clean)\n",
    "    over100=totalCrashes-within100-noCoords\n",
    "    over10=within100-within10\n",
    "    over1=within10-within1\n",
    "    over0=within1-within0\n",
    "    inBoundary=within0\n",
    "    \n",
    "    outputDataFrame.loc[len(outputDataFrame.index)]=[output_df_park,output_df_region,inBoundary,over0,over1,over10,over100,noCoords,totalCrashes]\n",
    "    \n",
    "    return outputDataFrame\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    outputDataFrame=pd.DataFrame(columns=[\"Park\",\"Region\",\"Within Boundary\",\"<1mi Outside\",\"1-10mi Outside\",\n",
    "                                          \"10-100mi Outside\",\">100mi Outside\",\"No Coordinates\",\"Total Crashes\"])\n",
    "    \n",
    "    ALLCRASHES_CLEAN=NCR_Cleaner(ALLCRASHES)\n",
    "    \n",
    "    for park in range(len(ALLBOUNDARIES)): #for every park in the full set of boundaries \n",
    "        \n",
    "        parkCode=ALLBOUNDARIES.loc[park][1] #take individual park code\n",
    "        \n",
    "        proj_park_crashes=crashChooser(ALLCRASHES_CLEAN,ALLBOUNDARIES,parkCode) #select park-specific crashes\n",
    "        proj_park_polygon=boundaryChooser(ALLCRASHES_CLEAN,ALLBOUNDARIES,parkCode) #select park-specific boundary(ies)\n",
    "        \n",
    "        output_df_park=proj_park_polygon.iloc[0][1] #select park code\n",
    "        output_df_region=str(proj_park_polygon.iloc[0][6])+\"R\" #select region code\n",
    "        \n",
    "        #Some AKR parks are recorded twice in an input dataset, must not record duplicates\n",
    "        \n",
    "        duplicate=output_df_park in outputDataFrame[\"Park\"].values\n",
    "        if duplicate==False:     \n",
    "        \n",
    "            if len(proj_park_crashes)==0: #if no crashes in a park, don't do spatial join calcs and add 0s to output df\n",
    "                \n",
    "                outputDataFrame.loc[len(outputDataFrame.index)]=[output_df_park,output_df_region,0,0,0,0,0,0,0]\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                outputDataFrame=calculations(proj_park_crashes, proj_park_polygon, outputDataFrame, output_df_park, output_df_region)\n",
    "            \n",
    "    #Output spreadsheet code here: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    outputDataFrame.to_excel(r\"C:\\Users\\Christopher.Dettmer\\Documents\\TSP\\Spatial Join Files\\8-25-22 Final Coordinate Stats and Charts.xlsx\",\n",
    "                             sheet_name=\"Output Data\", index = False)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833977de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd0bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
